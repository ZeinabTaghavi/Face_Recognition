{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1- Collecting your face images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface.xml')\n",
    "\n",
    "def face_extractor(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray , 1.3 ,5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_img = img[y:y+h,x:x+w]\n",
    "        \n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train'\n",
    "if not os.path.exists(train_path):\n",
    "    os.mkdir(train_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "img not found\n",
      "Collecting sample datas :)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "train_count = 0\n",
    "\n",
    "while True:\n",
    "    ret , frame = cap.read()\n",
    "    cropped_img = face_extractor(frame)\n",
    "    if cropped_img is not None:\n",
    "        train_count += 1\n",
    "        cropped_img = cv2.resize(cropped_img,(200,200))\n",
    "        cv2.imwrite(train_path+'/train_'+str(train_count)+'.jpg',cropped_img)\n",
    "        cv2.putText(cropped_img,str(train_count),(50,50),cv2.FONT_HERSHEY_COMPLEX,1, 150,2)\n",
    "        cv2.imshow('collecting train set',cropped_img)\n",
    "    else:\n",
    "        print('img not found')\n",
    "        \n",
    "    if cv2.waitKey(1)==13 or train_count == 500:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "print('Collecting sample datas :)')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train , labels = [],[]\n",
    "correct_files = [f for f in os.listdir(train_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , files in enumerate(correct_files):\n",
    "    img = cv2.imread(train_path + '/' + files , 0)\n",
    "    train.append(np.asarray(img, dtype=np.uint8))\n",
    "    labels.append(i)\n",
    "    \n",
    "labels = np.asarray(labels , dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model train is DONE :)\n"
     ]
    }
   ],
   "source": [
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.train(np.asarray(train),np.asarray(labels))\n",
    "print('model train is DONE :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341, 108.28643724891043)\n",
      "(39, 111.50394938260827)\n",
      "(302, 34.904066169044334)\n",
      "(40, 36.585424006063285)\n",
      "(370, 34.91743330591538)\n",
      "(14, 35.52601832390747)\n",
      "(37, 34.87088942629933)\n",
      "(14, 36.70319360817031)\n",
      "(484, 36.58183531181106)\n",
      "(484, 35.515807101576975)\n",
      "(14, 36.29593081267141)\n",
      "(14, 36.13356778381583)\n",
      "(484, 34.82640836097213)\n",
      "(484, 35.72623332957638)\n",
      "(484, 35.612144944768595)\n",
      "(484, 35.87317079952745)\n",
      "(484, 35.6959646411235)\n",
      "(484, 35.837171384713606)\n",
      "(484, 35.22654424794688)\n",
      "(484, 35.382581281513104)\n",
      "(480, 105.77439375238967)\n",
      "(480, 127.26673920070398)\n",
      "(298, 35.481408606680034)\n",
      "(302, 35.40648687225438)\n",
      "(321, 35.10059664320345)\n",
      "(10, 35.160780342146396)\n",
      "(10, 35.405182604023395)\n",
      "(321, 35.24293301066597)\n",
      "(385, 37.14373634394567)\n",
      "(267, 37.77121588106858)\n",
      "(442, 41.25190999617082)\n",
      "(266, 116.07799424772372)\n",
      "(201, 130.39001442178312)\n",
      "(442, 42.00347947166076)\n",
      "(442, 42.1211495938743)\n",
      "(391, 42.384553423847954)\n",
      "(391, 43.412818021060254)\n",
      "(391, 41.92317123494359)\n",
      "(442, 42.07270682984216)\n",
      "(442, 42.344219441570495)\n",
      "(391, 41.87215726017021)\n",
      "(341, 43.02038971231443)\n",
      "(391, 40.29281588982378)\n",
      "(391, 40.35690735175619)\n",
      "(391, 41.016281061017885)\n",
      "(442, 41.963822325959946)\n",
      "(341, 43.92630856995473)\n",
      "(341, 46.865984108076795)\n",
      "(265, 44.005220557565956)\n",
      "(265, 35.47329021643221)\n",
      "(384, 35.053297548109235)\n",
      "(384, 34.605415016911614)\n",
      "(384, 34.92476291960795)\n",
      "(234, 34.38031944548511)\n",
      "(51, 47.913013116088436)\n",
      "(486, 46.356831228385154)\n",
      "(266, 45.015202118326485)\n",
      "(39, 45.790222003524214)\n",
      "(266, 44.511779169749516)\n",
      "(266, 44.04446057061659)\n",
      "(266, 43.8214650829909)\n",
      "(266, 43.45393559811588)\n",
      "(266, 42.98010907037856)\n",
      "(442, 43.525259954629426)\n",
      "(442, 45.889491665843)\n",
      "(442, 42.70709268872582)\n",
      "(442, 46.13226702970302)\n",
      "(442, 43.962505362153195)\n",
      "(442, 45.58715801213357)\n",
      "(486, 42.92684604412215)\n",
      "(39, 40.48502027815399)\n",
      "(442, 40.35665813362832)\n",
      "(39, 40.77974795759248)\n",
      "(442, 41.71067650871695)\n",
      "(39, 41.18487291031532)\n",
      "(442, 40.303078843872576)\n",
      "(39, 41.03277366210882)\n",
      "(39, 41.87265892533338)\n",
      "(486, 44.57374621723187)\n",
      "(39, 106.22637895691406)\n",
      "(383, 106.60315349537657)\n",
      "(51, 90.638104082507)\n",
      "(234, 35.79863632915834)\n",
      "(442, 101.65561691984837)\n",
      "(415, 111.65837956127065)\n",
      "(480, 98.24738080160219)\n",
      "(39, 121.29402710297607)\n",
      "(3, 139.20579275094252)\n",
      "(15, 110.60841282154111)\n",
      "(442, 96.06518752962054)\n",
      "(341, 107.16094058019593)\n",
      "(442, 108.66304158219916)\n",
      "(341, 110.79659680567484)\n",
      "(20, 110.07376209735195)\n",
      "(341, 107.78553275750896)\n",
      "(14, 37.021034225237564)\n",
      "(442, 104.02632869092076)\n",
      "(265, 34.39028742646657)\n",
      "(40, 32.04026539417635)\n",
      "(442, 108.85595383590955)\n",
      "(265, 32.32443913064703)\n",
      "(442, 107.09638191358806)\n",
      "(40, 32.09279197535799)\n",
      "(51, 97.62233794295709)\n",
      "(234, 32.29763904983144)\n",
      "(302, 32.66275889186025)\n",
      "(302, 33.84657461249712)\n",
      "(341, 113.47667045060818)\n",
      "(301, 98.2943962543661)\n",
      "(51, 97.26042335088276)\n",
      "(267, 99.6564064982049)\n",
      "(463, 100.37647941186334)\n",
      "(51, 91.11367243434808)\n",
      "(367, 98.49876673261691)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while (True):\n",
    "    ret , frame = cap.read()\n",
    "    try:\n",
    "        face = face_extractor(frame)\n",
    "        face = cv2.cvtColor(face , cv2.COLOR_BGR2GRAY)\n",
    "        result = model.predict(face)\n",
    "\n",
    "        print(result)\n",
    "        lock = \"lock\"\n",
    "        if result[1] < 500:\n",
    "            conf = int(100 * (1 - (result[1]) / 300))\n",
    "            \n",
    "            if conf >= 88:\n",
    "                lock = \"unlock\"\n",
    "            display_result = str(conf) + '% confident it is User , \\n '+ lock\n",
    "            color = (0,255,0)\n",
    "            \n",
    "        else:\n",
    "            display_result = '0% confident it is User!'\n",
    "            color = (0,0,255)\n",
    "\n",
    "        cv2.putText(frame,display_result,(50,50) , cv2.FONT_HERSHEY_COMPLEX,2,color)\n",
    "    except:\n",
    "        cv2.putText(frame,'No Face',(50,50) , cv2.FONT_HERSHEY_COMPLEX,2,(0,0,255))\n",
    "        \n",
    "    cv2.imshow('Zeinab Detector',frame)\n",
    "    if cv2.waitKey(1) and 0xff==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
